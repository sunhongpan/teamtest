## 逻辑回归与线性回归
线性回归用于解决回归问题，而逻辑回归用于解决分类问题。  
线性回归：  
线性回归用于使用给定的一组独立变量来预测连续因变量。  
在线性回归中，我们预测连续变量的值。  
在线性回归中，我们找到了最佳拟合线，通过它我们可以轻松地预测输出。  
最小二乘估计法用于估计精度。  
线性回归的输出必须是一个连续值，例如价格，年龄等。  
在线性回归中，要求因变量和自变量之间的关系必须是线性的。  
在线性回归中，自变量之间可能存在共线性。  
逻辑回归：  
逻辑回归用于使用给定的一组独立变量来预测分类因变量。  
在逻辑回归中，我们预测类别变量的值。  
在逻辑回归中，我们找到了S曲线，可以用来对样本进行分类。  
最大似然估计方法用于估计准确性。  
逻辑回归的输出必须是分类值，例如0或1，是或否，等等。  
在逻辑回归中，不需要因变量和自变量之间具有线性关系。  
在逻辑回归中，自变量之间不应存在共线性。  

逻辑回归与线性回归的优缺点  
Logistic回归的优点：  
当数据集是线性可分离的时， Logistic回归表现良好。（对于非线性的数据集适应性弱）  
Logistic回归不太容易过拟合，但是在高维数据集中可能过拟合。您应该考虑使用正则化（L1和L2）技术，以避免在这些情况下过度拟合。  
Logistic回归不仅可以衡量预测变量（系数大小）的相关性，还可以给出其关联方向（正向或负向）。  
Logistic回归更易于实现，解释和培训，非常有效。  
Logistic回归的缺点：  
Logistic回归的主要局限性是假设因变量和自变量之间呈线性关系。在现实世界中，数据很少是线性可分离的。大多数时候，数据将是一团糟。  
如果观测数量少于要素数量，则不应使用Logistic回归，否则可能会导致过拟合。（当特征空间很大，性能欠佳）  
Logistic回归只能用于预测离散函数。因此，逻辑回归的因变量限于离散数集。这种限制本身是有问题的，因为它限制了连续数据的预测。  
因为预测结果呈Z字型（或反Z字型），因此当数据集中在中间区域时，对概率的变化会很敏感，可能导致预测结果缺乏区分度。  


1.先尝试调用sklearn的线性回归模型训练数据，尝试以下代码，画图查看分类的结果


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from __future__ import print_function
import numpy as np
from sklearn.linear_model import LogisticRegression

df_X = pd.read_csv('./logistic_x.txt', sep='\ +',header=None, engine='python') #读取X值
ys = pd.read_csv('./logistic_y.txt', sep='\ +',header=None, engine='python') #读取y值
ys = ys.astype(int)
df_X['label'] = ys[0].values #将X按照y值的结果一一打标签

ax = plt.axes()
#在二维图中描绘X点所处位置，直观查看数据点的分布情况
df_X.query('label == 0').plot.scatter(x=0, y=1, ax=ax, color='blue')
df_X.query('label == 1').plot.scatter(x=0, y=1, ax=ax, color='red')

#提取用于学习的数据
Xs = df_X[[0, 1]].values
Xs = np.hstack([np.ones((Xs.shape[0], 1)), Xs]) 
ys = df_X['label'].values

lr = LogisticRegression(fit_intercept=False) #因为前面已经将截距项的值合并到变量中，此处参数设置不需要截距项
lr.fit(Xs, ys) #拟合
score = lr.score(Xs, ys) #结果评价
print("Coefficient: %s" % lr.coef_) # Coefficient: [[-1.70090714  0.55446484  1.07222372]]
print("Score: %s" % score) # Score: 0.898989898989899

ax = plt.axes()

df_X.query('label == 0').plot.scatter(x=0, y=1, ax=ax, color='blue')
df_X.query('label == 1').plot.scatter(x=0, y=1, ax=ax, color='red')

_xs = np.array([np.min(Xs[:,1]), np.max(Xs[:,1])])

#将数据以二维图形式描点，并用学习得出的参数结果作为阈值，划分数据区域
_ys = (lr.coef_[0][0] + lr.coef_[0][1] * _xs) / (- lr.coef_[0][2])
plt.plot(_xs, _ys, lw=1)
```

2.用梯度下降法将相同的数据分类，画图和sklearn的结果相比较



```python
class LGR_NT():
    def __init__(self):
        self.w = None
        self.n_iters = None
    def fit(self,X,y,loss = 1e-10): # 判断是否收敛的条件为1e-10
        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算
        [m,d] = np.shape(X) #自变量的维度
        self.w = np.zeros((1,d)) #将参数的初始值定为0
        tol = 1e5
        n_iters =0
        Hessian = np.zeros((d,d))

        while tol > loss:
            zs = X.dot(self.w.T)
            h_f = 1 / (1 + np.exp(-zs))
            grad = np.mean(X*(y - h_f),axis=0)
            for i in range(d):
                for j in range(d):
                    if j>=i:
                        Hessian[i][j] = np.mean(h_f*(h_f-1)*X[:,i]*X[:,j]) #更新海森矩阵中的值
                    else:
                        Hessian[i][j] = Hessian[j][i] #按海森矩阵的性质，对称点可直接得出结果
            theta = self.w - np.linalg.inv(Hessian).dot(grad)
            tol = np.sum(np.abs(theta - self.w))
            self.w = theta
            n_iters += 1

        self.w = theta
        self.n_iters = n_iters

    def predict(self, X):
        # 用已经拟合的参数值预测新自变量
        y_pred = X.dot(self.w)
        return y_pred  

if __name__ == "__main__":
    lgr_nt = LGR_NT()
    lgr_nt.fit(Xs,ys)
```

3.用牛顿法实现结果，画图和sklearn的结果相比较，并比较牛顿法和梯度下降法迭代收敛的次数


```python
class LGR_NT():
    def __init__(self):
        self.w = None
        self.n_iters = None
    def fit(self,X,y,loss = 1e-10): # 判断是否收敛的条件为1e-10
        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算
        [m,d] = np.shape(X) #自变量的维度
        self.w = np.zeros((1,d)) #将参数的初始值定为0
        tol = 1e5
        n_iters =0
        Hessian = np.zeros((d,d))

        while tol > loss:
            zs = X.dot(self.w.T)
            h_f = 1 / (1 + np.exp(-zs))
            grad = np.mean(X*(y - h_f),axis=0)
            for i in range(d):
                for j in range(d):
                    if j>=i:
                        Hessian[i][j] = np.mean(h_f*(h_f-1)*X[:,i]*X[:,j]) #更新海森矩阵中的值
                    else:
                        Hessian[i][j] = Hessian[j][i] #按海森矩阵的性质，对称点可直接得出结果
            theta = self.w - np.linalg.inv(Hessian).dot(grad)
            tol = np.sum(np.abs(theta - self.w))
            self.w = theta
            n_iters += 1

        self.w = theta
        self.n_iters = n_iters

    def predict(self, X):
        # 用已经拟合的参数值预测新自变量
        y_pred = X.dot(self.w)
        return y_pred  

if __name__ == "__main__":
    lgr_nt = LGR_NT()
    lgr_nt.fit(Xs,ys)
```

4.比较梯度下降法和牛顿法收敛速度


```python
print("梯度下降法结果参数：%s;梯度下降法迭代次数：%s" %(lr_gd.w,lr_gd.n_iters))
print("牛顿法结果参数：%s;牛顿法迭代次数：%s" %(lgr_nt.w,lgr_nt.n_iters))
```


```python

```


```python

```
